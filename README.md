# Training_Word2Vec_Embbeddings_for_ISNA_News
In this repository, I trained the embedding of words (Word2Vec representation) to evaluate the similarity of word embeddings and get an intuition about Persian close-meaning words, using the ISNA news corpus, "PERSICA", as the dataset. The "PERSICA" corpus includes news ID, title, text, date, publication time, and the news label. It can be used for several NLP tasks like news classification, topic discovery and classification, category classification, and many more procedures.
The dataset is an open-source corpus that can be downloaded from the address (https://sourceforge.net/projects/persica/files/persica.csv/download). In the Notebook code I provided, I wrote step-by-step procedures that should be done to train word vectors (Word2Vec). These steps are as follows:
The body of this Notebook code which was written in the Colab environment, consists of the following 7 headings:

Step 1Ô∏è‚É£: In the first step, by defining the functions "read_isna_content()" and "read_isna_content_to_list()", the text content of all corpus news has been read, and a list of all ISNA corpus news is returned as the output of the function.üïê
Step 2: In the 


.
